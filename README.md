
# ETL pipeline project

This ETL pipeline project uses PySpark and PostgreSQL for data processing. A Spark session is created with PostgreSQL configurations, and data is extracted from the movies and users tables into Spark DataFrames. The data is transformed by calculating average movie ratings and joining this with movie details. The transformed data is then loaded back into the PostgreSQL database into an avg_ratings table. This project highlights the use of PySpark for efficient data processing and PostgreSQL for storage and retrieval, demonstrating skills in ETL processes within a distributed computing environment.
##  Programming Languages 👨‍💻

- Python
## Process & Technologies Used 📜


- PySpark for data processing and transformation.

- PostgreSQL as the data source and destination.

- SparkSession for managing the Spark application.

- JDBC (Java Database Connectivity) for connecting Spark to the PostgreSQL database
## Skills Obtained 🔑

- PySpark Proficiency

- Data Extraction
- Data Loading

- Database Management
- Data Engineering



## Authors 🧑‍🏫

- Harshit Tyagi

